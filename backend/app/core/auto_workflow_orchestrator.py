"""
Auto-Workflow-Orchestrator fÃ¼r Xionimus AI
Automatisiert den kompletten Workflow von Research â†’ KlÃ¤rung â†’ Code-Generierung
"""
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import re

logger = logging.getLogger(__name__)

class AutoWorkflowOrchestrator:
    """
    Orchestriert automatischen Workflow ohne User-Input
    Research â†’ Auto-KlÃ¤rungsfragen â†’ Code-Generierung
    """
    
    def __init__(self):
        self.workflow_state: Dict[str, Any] = {}
    
    async def should_auto_continue(
        self,
        ai_response: str,
        conversation_history: List[Dict[str, str]]
    ) -> Optional[Dict[str, Any]]:
        """
        PrÃ¼ft, ob automatische Fortsetzung nÃ¶tig ist
        
        Returns:
            Dict mit auto_continue action oder None
        """
        # Check 1: Research abgeschlossen + KlÃ¤rungsfragen vorhanden
        if self._has_research_and_questions(ai_response):
            logger.info("ðŸŽ¯ Research + KlÃ¤rungsfragen erkannt â†’ Auto-Continue")
            return {
                'action': 'auto_answer_clarifications',
                'reason': 'Research abgeschlossen mit KlÃ¤rungsfragen',
                'original_request': self._extract_original_request(conversation_history)
            }
        
        # Check 2: KlÃ¤rungsfragen beantwortet â†’ Code-Generierung
        if self._questions_answered_waiting_for_code(ai_response):
            logger.info("ðŸŽ¯ KlÃ¤rungsfragen beantwortet â†’ Auto-Code-Generierung")
            return {
                'action': 'auto_generate_code',
                'reason': 'KlÃ¤rungsfragen beantwortet, bereit fÃ¼r Code',
                'requirements': self._extract_requirements(conversation_history)
            }
        
        return None
    
    def _has_research_and_questions(self, text: str) -> bool:
        """PrÃ¼ft ob Research + KlÃ¤rungsfragen vorhanden"""
        indicators = [
            'recherche abgeschlossen' in text.lower(),
            'research completed' in text.lower(),
            'klÃ¤rungsfragen' in text.lower(),
            'clarifying questions' in text.lower(),
            'basierend auf dieser recherche' in text.lower()
        ]
        
        # Muss Research + Fragen haben
        has_research = any([
            'recherche' in text.lower(),
            'research' in text.lower()
        ])
        
        has_questions = any([
            '?' in text,
            'frage' in text.lower(),
            'question' in text.lower()
        ])
        
        return has_research and has_questions and any(indicators)
    
    def _questions_answered_waiting_for_code(self, text: str) -> bool:
        """PrÃ¼ft ob Fragen beantwortet und auf Code wartet"""
        # Noch nicht implementiert - fÃ¼r zukÃ¼nftige Erweiterung
        return False
    
    def _extract_original_request(self, history: List[Dict[str, str]]) -> str:
        """Extrahiert ursprÃ¼ngliche Coding-Anfrage"""
        # Suche letzte User-Message vor Research
        for msg in reversed(history):
            if msg.get('role') == 'user':
                content = msg.get('content', '')
                # Skip "fahre fort" oder Ã¤hnliche Meta-Commands
                if len(content) > 30 and not any(skip in content.lower() for skip in ['fahre fort', 'continue', 'weiter']):
                    return content
        return ""
    
    def _extract_requirements(self, history: List[Dict[str, str]]) -> Dict[str, Any]:
        """Extrahiert Requirements aus Conversation"""
        requirements = {
            'features': [],
            'tech_stack': [],
            'constraints': []
        }
        # Placeholder fÃ¼r zukÃ¼nftige Implementierung
        return requirements
    
    async def auto_answer_clarifications(
        self,
        research_content: str,
        clarification_questions: str,
        original_request: str,
        ai_manager
    ) -> str:
        """
        Beantwortet KlÃ¤rungsfragen automatisch basierend auf Best Practices
        
        Args:
            research_content: Research-Ergebnisse
            clarification_questions: Die gestellten Fragen
            original_request: UrsprÃ¼ngliche User-Anfrage
            ai_manager: AI Manager fÃ¼r LLM-Calls
            
        Returns:
            Automatisch generierte Antworten
        """
        logger.info("ðŸ¤– Generiere automatische Antworten auf KlÃ¤rungsfragen...")
        
        # Erstelle Prompt fÃ¼r automatische Beantwortung
        auto_answer_prompt = f"""Als Experte fÃ¼r Software-Entwicklung, beantworte die folgenden KlÃ¤rungsfragen AUTOMATISCH basierend auf Best Practices und modernen Standards.

**UrsprÃ¼ngliche Anfrage:**
{original_request}

**Research-Ergebnisse:**
{research_content[:1000]}...

**KlÃ¤rungsfragen:**
{clarification_questions}

**Deine Aufgabe:**
Beantworte ALLE Fragen prÃ¤zise und praxisorientiert. WÃ¤hle moderne, bewÃ¤hrte Technologien und Best Practices.

**Antwort-Format:**
FÃ¼r jede Frage:
1. [Frage-Nummer] **Kurze Antwort**
   - BegrÃ¼ndung in 1-2 SÃ¤tzen

**Richtlinien:**
- WÃ¤hle moderne Tech-Stack (React 18+, Node.js, TypeScript wenn mÃ¶glich)
- Bevorzuge populÃ¤re, gut dokumentierte Libraries
- Fokus auf Developer Experience und Maintainability
- Setze auf bewÃ¤hrte Patterns (REST API, Component-based)
- Implementiere Best Practices (Error-Handling, Testing, Security)

Gebe NUR die Antworten, keine weitere ErklÃ¤rung."""

        try:
            # Verwende Claude fÃ¼r intelligente Beantwortung
            response = await ai_manager.generate_response(
                provider="anthropic",
                model="claude-sonnet-4-5-20250929",
                messages=[{"role": "user", "content": auto_answer_prompt}],
                stream=False,
                temperature=0.7
            )
            
            answers = response.get("content", "")
            logger.info(f"âœ… Automatische Antworten generiert: {len(answers)} Zeichen")
            return answers
            
        except Exception as e:
            logger.error(f"âŒ Fehler bei Auto-Beantwortung: {e}")
            # Fallback: Generische Best-Practice Antworten
            return self._generate_fallback_answers()
    
    def _generate_fallback_answers(self) -> str:
        """Generiert generische Best-Practice Antworten als Fallback"""
        return """Basierend auf modernen Best Practices:

1. **Programmiersprache/Framework**: React 18 mit TypeScript, Node.js Backend
2. **Architektur**: Full-Stack (React Frontend + REST API Backend)
3. **Design**: Material-UI oder Chakra UI fÃ¼r modernes, responsives Design
4. **Authentifizierung**: JWT-basiert mit Refresh Tokens
5. **Datenbank**: MongoDB oder PostgreSQL (basierend auf Anforderungen)
6. **Features**: VollstÃ¤ndige CRUD-Operationen, Real-time Updates, Error-Handling"""
    
    async def execute_auto_workflow(
        self,
        workflow_action: Dict[str, Any],
        ai_manager,
        api_keys: Dict[str, str]
    ) -> Dict[str, Any]:
        """
        FÃ¼hrt automatischen Workflow aus
        
        Returns:
            Dict mit Ergebnis und nÃ¤chster Message
        """
        action = workflow_action['action']
        
        if action == 'auto_answer_clarifications':
            # Schritt 1: Beantworte Fragen automatisch
            # Schritt 2: Generiere Code direkt
            logger.info("ðŸš€ Starte Auto-Workflow: Research â†’ Antworten â†’ Code")
            
            return {
                'success': True,
                'next_message': 'Basierend auf Best Practices starte ich nun automatisch mit der Code-Generierung...',
                'should_generate_code': True
            }
        
        return {
            'success': False,
            'next_message': None
        }

# Global instance
auto_workflow_orchestrator = AutoWorkflowOrchestrator()
