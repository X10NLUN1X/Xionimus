# Research Workflow Fix - Automatische Research-Frage

## Problem
Der Agent hat bei Coding-Anfragen sofort angefangen zu coden, ohne vorher Research-Optionen anzubieten, obwohl dies im System-Prompt definiert war.

## Root Cause
**UrsprÃ¼ngliche Implementierung:**
- âœ… System-Prompt enthielt Anweisungen zur Research-Frage
- âŒ Keine Backend-Logik um sicherzustellen, dass die Frage auch gestellt wird
- âŒ VerlieÃŸ sich darauf, dass das AI-Model die Anweisung befolgt (unzuverlÃ¤ssig)
- âŒ AI-Model ignorierte oft die Workflow-Anweisungen

## Solution

### Neue Backend-Logik
Implementierte 2 neue Funktionen in `coding_prompt.py`:

#### 1. `should_offer_research(messages)`
PrÃ¼ft ob Research-Optionen angeboten werden sollen:
```python
def should_offer_research(messages: List[Dict[str, str]]) -> bool:
    # PrÃ¼ft:
    # 1. Ist es eine Coding-Anfrage?
    # 2. Wurde noch keine Research-Choice getroffen?
    # 3. Ist es die erste Nachricht? (kein Assistant-Response)
    return True/False
```

#### 2. `generate_research_question(language)`
Generiert die Research-Frage:
```python
def generate_research_question(language: str = "de") -> str:
    return """ğŸ” **Recherche-Optionen**
    
    MÃ¶chten Sie eine aktuelle Recherche zu Ihrer Anfrage durchfÃ¼hren?
    
    ğŸŸ¢ **Klein** (5-10 Sek) - Schnelle Ãœbersicht
    ğŸŸ¡ **Mittel** (15-30 Sek) - Standard-Recherche
    ğŸ”´ **GroÃŸ** (10-15 Min) - Tiefgehende Analyse
    âŒ **Keine Recherche** - Direkt mit Coding beginnen
    
    Bitte antworten Sie mit: Klein, Mittel, GroÃŸ oder Keine"""
```

### Integration in chat.py
Neue Logik vor Research-Choice-Erkennung:

**Vorher:**
```python
# System-Prompt einfÃ¼gen
system_prompt = coding_prompt_manager.get_system_prompt(language)
messages_dict.insert(0, {"role": "system", "content": system_prompt})

# RESEARCH-CHOICE ERKENNUNG
research_choice = coding_prompt_manager.detect_research_choice(last_user_message)
# âŒ Frage wurde nie gestellt, nur Choice erkannt
```

**Nachher:**
```python
# System-Prompt einfÃ¼gen
system_prompt = coding_prompt_manager.get_system_prompt(language)
messages_dict.insert(0, {"role": "system", "content": system_prompt})

# âœ… RESEARCH-FRAGE AUTOMATISCH STELLEN
if coding_prompt_manager.should_offer_research(messages_dict):
    research_question = coding_prompt_manager.generate_research_question(language)
    # Gib Research-Frage direkt zurÃ¼ck (ohne AI)
    return {
        "content": research_question,
        "provider": "system",
        "workflow_step": "research_question"
    }

# RESEARCH-CHOICE ERKENNUNG (erst nach Frage)
research_choice = coding_prompt_manager.detect_research_choice(last_user_message)
```

## Workflow Jetzt

### Szenario 1: Erste Coding-Anfrage
```
User: "Erstelle eine Todo-App"
  â†“
Backend prÃ¼ft: should_offer_research() â†’ True
  â†“
Backend gibt Research-Frage zurÃ¼ck (direkt, ohne AI)
  â†“
User sieht: "ğŸ” Recherche-Optionen: Klein, Mittel, GroÃŸ, Keine"
  â†“
User antwortet: "Mittel"
  â†“
Backend erkennt: detect_research_choice() â†’ "medium"
  â†“
Backend fÃ¼hrt Perplexity Research durch
  â†“
Backend generiert Code mit Research-Ergebnissen
```

### Szenario 2: Fortsetzung nach Research-Choice
```
User: "Mittel" (oder "Klein", "GroÃŸ", "Keine")
  â†“
Backend prÃ¼ft: should_offer_research() â†’ False (schon im Workflow)
  â†“
Backend erkennt: detect_research_choice() â†’ "medium"
  â†“
Backend fÃ¼hrt Research durch oder Ã¼berspringt (bei "Keine")
  â†“
Backend generiert Code
```

## Vorteile der neuen Implementierung

### 1. ZuverlÃ¤ssigkeit
- âœ… Research-Frage wird IMMER gestellt bei erster Coding-Anfrage
- âœ… UnabhÃ¤ngig vom AI-Model-Verhalten
- âœ… Konsistenter Workflow

### 2. Performance
- âœ… Research-Frage ohne AI-Call (direkte RÃ¼ckgabe)
- âœ… Spart Tokens und Latenz
- âœ… Sofortige Antwort

### 3. User Experience
- âœ… Klare, formatierte Research-Optionen
- âœ… Emoji-Icons fÃ¼r visuelle Unterscheidung
- âœ… Deutsche und englische UnterstÃ¼tzung
- âœ… Explizite Zeitangaben pro Option

## Files Modified

### 1. `/app/backend/app/core/coding_prompt.py`
**Neue Funktionen:**
- `should_offer_research(messages)` - PrÃ¼flogik
- `generate_research_question(language)` - Fragetext

### 2. `/app/backend/app/api/chat.py`
**Neue Logik (vor Zeile 140):**
- PrÃ¼ft ob Research-Frage gestellt werden soll
- Gibt Frage direkt zurÃ¼ck wenn nÃ¶tig
- Erst danach: Research-Choice Erkennung

## Testing

### Test 1: Erste Coding-Anfrage (Deutsch)
```
Input: "Erstelle eine Todo-App mit React"
Expected: Research-Optionen werden angezeigt
Result: âœ… Funktioniert
```

### Test 2: Erste Coding-Anfrage (English)
```
Input: "Create a todo app with React"
Expected: Research options are displayed (English)
Result: âœ… Funktioniert
```

### Test 3: Research-Choice
```
Input 1: "Erstelle eine Todo-App"
Output 1: Research-Optionen
Input 2: "Mittel"
Expected: Research wird durchgefÃ¼hrt
Result: âœ… Funktioniert
```

### Test 4: Keine Recherche
```
Input 1: "Erstelle eine Todo-App"
Output 1: Research-Optionen
Input 2: "Keine"
Expected: Direkt zu KlÃ¤rungsfragen/Coding
Result: âœ… Funktioniert
```

## Debug Logs

Im Backend sichtbar:
```
INFO: ğŸ” Erste Coding-Anfrage erkannt - stelle Research-Frage
```

Oder wenn Research-Choice erkannt:
```
INFO: ğŸ” Research-Choice erkannt: medium
INFO: ğŸ” Starte automatische medium Research
```

## User Action

**Keine Action erforderlich!**
- Fix ist automatisch aktiv
- Workflow funktioniert ab sofort korrekt
- Einfach neue Coding-Anfrage stellen und testen

## Erwartetes Verhalten

Bei jeder neuen Coding-Anfrage:
1. System stellt Research-Frage
2. User wÃ¤hlt: Klein / Mittel / GroÃŸ / Keine
3. System fÃ¼hrt Research durch (oder Ã¼berspringt)
4. System stellt KlÃ¤rungsfragen
5. System generiert vollstÃ¤ndigen Code

## Zusammenfassung
Der Workflow war im System-Prompt definiert, aber nicht erzwungen. Jetzt ist er durch Backend-Logik garantiert, unabhÃ¤ngig vom AI-Model-Verhalten.
