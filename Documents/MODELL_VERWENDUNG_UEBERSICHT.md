# ğŸ¤– Modell-Verwendung in Xionimus AI - Detaillierte Ãœbersicht

## ğŸ“‹ INHALTSVERZEICHNIS
1. [Standard-Modelle](#standard-modelle)
2. [Automatische Modellauswahl](#automatische-modellauswahl)
3. [Intelligente Agent-Zuordnung](#intelligente-agent-zuordnung)
4. [Research-System](#research-system)
5. [Spezielle Operationen](#spezielle-operationen)
6. [Multi-Agent System](#multi-agent-system)
7. [Kostenoptimierung](#kostenoptimierung)

---

## 1ï¸âƒ£ STANDARD-MODELLE

### **Benutzer Chat (Keine Auswahl)**
Wenn Benutzer KEIN spezifisches Modell wÃ¤hlt:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Standard-Modell: gpt-4o-mini                    â”‚
â”‚ Provider: OpenAI                                â”‚
â”‚ Kosten: $0.38 pro 1M Tokens                    â”‚
â”‚ Verwendung: 90% aller Chat-Anfragen            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Definiert in:** `/app/backend/app/api/chat.py` Zeile 48
```python
model: str = Field(default="gpt-4o-mini", ...)
```

---

## 2ï¸âƒ£ AUTOMATISCHE MODELLAUSWAHL

### **Intelligente Task-Erkennung**

Das System analysiert die Benutzeranfrage und wÃ¤hlt automatisch das beste Modell:

#### **A) Allgemeine Konversation**
```
Beispiele:
- "Hallo, wie geht's?"
- "ErklÃ¤re mir Quantenphysik"
- "Was ist der Sinn des Lebens?"

Modell: gpt-4o
Provider: OpenAI
Temperatur: 0.8 (kreativ)
Kosten: $6.25/1M Tokens
```

#### **B) Code-Analyse**
```
Beispiele:
- "Analysiere diesen Code"
- "Was macht diese Funktion?"
- "Code Review fÃ¼r..."

Modell: claude-sonnet-4-5-20250929
Provider: Anthropic
Temperatur: 0.3 (prÃ¤zise)
Kosten: $9.00/1M Tokens
```

#### **C) Komplexes Reasoning**
```
Beispiele:
- "Analysiere die Architektur"
- "Vergleiche diese AnsÃ¤tze"
- "Evaluiere die LÃ¶sung"

Modell: claude-sonnet-4-5-20250929
Provider: Anthropic
Temperatur: 0.5
Kosten: $9.00/1M Tokens
```

#### **D) Web Research**
```
Beispiele:
- "Suche nach aktuellen Trends"
- "Was sind die neuesten Features?"
- "Recherchiere zu..."

Modell: sonar-pro
Provider: Perplexity
Temperatur: 0.6
Kosten: $9.00/1M Tokens
```

#### **E) Debugging**
```
Beispiele:
- "Debug diesen Error"
- "Warum funktioniert das nicht?"
- "Fix this bug"

Modell: claude-opus-4-1-20250805
Provider: Anthropic
Temperatur: 0.3
Kosten: ~$15.00/1M Tokens (Opus)
```

#### **F) System-Analyse**
```
Beispiele:
- "Analysiere die System-Architektur"
- "Bewerte die Performance"
- "Optimiere das System"

Modell: claude-opus-4-1-20250805
Provider: Anthropic
Temperatur: 0.4
Kosten: ~$15.00/1M Tokens (Opus)
```

#### **G) Kreatives Schreiben**
```
Beispiele:
- "Schreibe eine Geschichte"
- "Erstelle einen Blogpost"
- "Generiere kreative Ideen"

Modell: gpt-4o
Provider: OpenAI
Temperatur: 0.9 (sehr kreativ)
Kosten: $6.25/1M Tokens
```

#### **H) Technische Dokumentation**
```
Beispiele:
- "Erstelle eine README"
- "Schreibe die API-Dokumentation"
- "Dokumentiere den Code"

Modell: claude-sonnet-4-5-20250929
Provider: Anthropic
Temperatur: 0.4
Kosten: $9.00/1M Tokens
```

**Definiert in:** `/app/backend/app/core/intelligent_agents.py`

---

## 3ï¸âƒ£ INTELLIGENTE AGENT-ZUORDNUNG

### **Keyword-basierte Erkennung**

Das System scannt Benutzeranfragen nach SchlÃ¼sselwÃ¶rtern:

#### **Code-Keywords â†’ Claude Sonnet**
```
Keywords: code, function, bug, error, debug, 
          programming, script, api, class, method

â†’ claude-sonnet-4-5-20250929 ($9.00/1M)
```

#### **Reasoning-Keywords â†’ Claude Sonnet**
```
Keywords: analyze, explain, why, how, compare, 
          evaluate, assess, reasoning

â†’ claude-sonnet-4-5-20250929 ($9.00/1M)
```

#### **Research-Keywords â†’ Perplexity**
```
Keywords: search, find, research, latest, current, 
          news, information, data, internet, web, 
          suche, recherche, aktuell

â†’ sonar-pro ($9.00/1M) oder sonar ($0.20/1M)
```

#### **Debugging-Keywords â†’ Claude Opus**
```
Keywords: debug, fix, error, issue, problem, 
          not working, broken, crash, fehler

â†’ claude-opus-4-1-20250805 ($15.00/1M)
```

#### **Creative-Keywords â†’ GPT-4o**
```
Keywords: write, story, creative, blog, article, 
          content, generate, create, schreiben

â†’ gpt-4o ($6.25/1M)
```

**Definiert in:** `/app/backend/app/core/intelligent_agents.py` Zeile 91-130

---

## 4ï¸âƒ£ RESEARCH-SYSTEM

### **Automatische Research-Modellauswahl**

Wenn Benutzer Research wÃ¤hlt (Klein/Mittel/GroÃŸ):

#### **Klein (Small) - Schnelle Ãœbersicht**
```
Modell: sonar
Provider: Perplexity
Kosten: $0.20/1M Tokens â­
Verwendung: Grundlegende Infos, Quick Facts
Prompt: "Schnelle Ãœbersicht mit Best Practices"
```

#### **Mittel (Medium) - Standard Research**
```
Modell: sonar-pro
Provider: Perplexity
Kosten: $9.00/1M Tokens
Verwendung: Detaillierte Recherche, Code-Beispiele
Prompt: "Detaillierte Recherche mit Best Practices 2025"
```

#### **GroÃŸ (Large) - Tiefgehende Analyse**
```
Modell: sonar-deep-research
Provider: Perplexity
Kosten: $12.50/1M Tokens
Verwendung: Umfassende Analyse, Production Patterns
Prompt: "Tiefgehende Analyse mit Performance-Vergleichen"
```

#### **Auto - Automatische KomplexitÃ¤tsberechnung**
```
System berechnet KomplexitÃ¤t basierend auf:
- Anzahl Technologien erwÃ¤hnt
- LÃ¤nge der Beschreibung
- Feature-KomplexitÃ¤t

KomplexitÃ¤t < 3 â†’ Klein (sonar)
KomplexitÃ¤t 3-6 â†’ Mittel (sonar-pro)
KomplexitÃ¤t > 6 â†’ GroÃŸ (sonar-deep-research)
```

**Definiert in:** `/app/backend/app/core/coding_prompt.py` Zeile 184-213

---

## 5ï¸âƒ£ SPEZIELLE OPERATIONEN

### **A) Test-Generierung**
```
Operation: Automatische Test-Code Generierung
Modell: claude-haiku-3.5-20241022 â­
Kosten: $2.40/1M Tokens (73% gÃ¼nstiger!)
Grund: Tests brauchen nicht hÃ¶chste Intelligenz
```

**Code-Stelle:** `/app/backend/app/api/chat.py` Zeile 589-595

---

### **B) Session Summaries (Fork-Funktion)**

#### **Summary Generation**
```
Operation: Zusammenfassung der Chat-Session
Modell: claude-haiku-3.5-20241022 â­
Kosten: $2.40/1M Tokens (73% gÃ¼nstiger!)
Prompt: "Analysiere Session und erstelle Zusammenfassung"
```

#### **Next Steps Generation**
```
Operation: 3 nÃ¤chste Schritte vorschlagen
Modell: claude-haiku-3.5-20241022 â­
Kosten: $2.40/1M Tokens
Format: JSON mit title, description, action
```

**Definiert in:** `/app/backend/app/api/session_management.py` Zeile 211-263

---

### **C) Post-Code Suggestions**

```
Operation: VerbesserungsvorschlÃ¤ge nach Code-Generierung
Modell: Gleich wie verwendetes Haupt-Modell
Kondition: Nur bei erfolgreicher Code-Generierung
Beispiele: "Tests hinzufÃ¼gen", "Fehlerbehandlung", "Dokumentation"
```

**Definiert in:** `/app/backend/app/api/chat.py` Zeile 650-680

---

## 6ï¸âƒ£ MULTI-AGENT SYSTEM

**Status:** Implementiert, aber standardmÃ¤ÃŸig DEAKTIVIERT

Wenn aktiviert (`multi_agent_mode: true`):

### **Agent-Rollen & Modelle**

#### **1. ARCHITECT Agent**
```
Aufgabe: System-Design, Architektur-Planung
Modell: claude-opus-4-1-20250805
Kosten: $15.00/1M Tokens
Temperatur: 0.4
PrioritÃ¤t: 10 (hÃ¶chste)
```

#### **2. ENGINEER Agent**
```
Aufgabe: Code-Implementierung
Modell: claude-sonnet-4-5-20250929
Kosten: $9.00/1M Tokens
Temperatur: 0.3
PrioritÃ¤t: 9
```

#### **3. UI_UX Agent**
```
Aufgabe: Frontend-Design, BenutzeroberflÃ¤che
Modell: gpt-4o
Kosten: $6.25/1M Tokens
Temperatur: 0.7
PrioritÃ¤t: 8
```

#### **4. TESTER Agent**
```
Aufgabe: Test-Code Generierung
Modell: claude-haiku-3.5-20241022 â­
Kosten: $2.40/1M Tokens
Temperatur: 0.3
PrioritÃ¤t: 7
```

#### **5. DEBUGGER Agent**
```
Aufgabe: Fehleranalyse, Debugging
Modell: claude-opus-4-1-20250805
Kosten: $15.00/1M Tokens
Temperatur: 0.2
PrioritÃ¤t: 10
```

#### **6. DOCUMENTER Agent**
```
Aufgabe: Dokumentation erstellen
Modell: gpt-4o
Kosten: $6.25/1M Tokens
Temperatur: 0.5
PrioritÃ¤t: 6
```

**Definiert in:** `/app/backend/app/core/multi_agent_orchestrator.py`

**Aktivierung:** Nur wenn `multi_agent_mode: true` in Request

---

## 7ï¸âƒ£ KOSTENOPTIMIERUNG

### **Optimierte Standard-Einstellungen**

#### **Vorher (Teuer):**
```
Standard: gpt-5 (nicht verfÃ¼gbar) â†’ $37.50/1M
Test-Gen: claude-sonnet â†’ $9.00/1M
Summaries: claude-sonnet â†’ $9.00/1M
Research: sonar-pro â†’ $9.00/1M
```

#### **Jetzt (GÃ¼nstig):** â­
```
Standard: gpt-4o-mini â†’ $0.38/1M (94% gÃ¼nstiger!)
Test-Gen: claude-haiku â†’ $2.40/1M (73% gÃ¼nstiger!)
Summaries: claude-haiku â†’ $2.40/1M (73% gÃ¼nstiger!)
Research: sonar â†’ $0.20/1M (98% gÃ¼nstiger!)
```

### **Durchschnittliche Kostenverteilung**

Bei 100 Benutzer-Chats mit Standard-Einstellungen:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Operation           â”‚ Modell        â”‚ Kosten   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 80 Normal Chats     â”‚ gpt-4o-mini   â”‚ $0.061  â”‚
â”‚ 10 Research         â”‚ sonar         â”‚ $0.004  â”‚
â”‚ 5 Code-Analyse      â”‚ haiku         â”‚ $0.024  â”‚
â”‚ 3 Summaries         â”‚ haiku         â”‚ $0.014  â”‚
â”‚ 2 Test-Gen          â”‚ haiku         â”‚ $0.010  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GESAMT                              â”‚ $0.113  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

VS. Alte Konfiguration: $1.50
ERSPARNIS: 92.5%! ğŸ‰
```

---

## 8ï¸âƒ£ FALLBACK-STRATEGIE

### **Wenn bevorzugter Provider nicht verfÃ¼gbar:**

#### **OpenAI nicht verfÃ¼gbar:**
```
1. Versuch: Anthropic (Claude)
2. Versuch: Perplexity (Sonar)
```

#### **Anthropic nicht verfÃ¼gbar:**
```
1. Versuch: OpenAI (GPT)
2. Versuch: Perplexity (Sonar)
```

#### **Perplexity nicht verfÃ¼gbar:**
```
1. Versuch: OpenAI (GPT)
2. Versuch: Anthropic (Claude)
```

**Definiert in:** `/app/backend/app/core/intelligent_agents.py` Zeile 85-89

---

## 9ï¸âƒ£ BENUTZER-STEUERUNG

### **Wie Benutzer Modelle auswÃ¤hlen:**

#### **Option 1: Automatisch (Standard)**
```
System wÃ¤hlt basierend auf Task-Type
âœ… Empfohlen fÃ¼r beste Kosten/QualitÃ¤t
```

#### **Option 2: Manuell in Settings**
```
Benutzer wÃ¤hlt:
- Provider (OpenAI/Anthropic/Perplexity)
- Modell (gpt-4o-mini, claude-haiku, etc.)
- System verwendet diese Auswahl
```

#### **Option 3: Pro Anfrage**
```
API-Request enthÃ¤lt:
{
  "provider": "openai",
  "model": "gpt-4o-mini",
  ...
}
```

---

## ğŸ¯ ZUSAMMENFASSUNG

### **Standard-Workflow (90% der Anfragen):**

```
1. Benutzer stellt Anfrage
   â†“
2. System erkennt Task-Type
   â†“
3. Intelligente Modellauswahl:
   - Allgemein â†’ gpt-4o-mini ($0.38/1M) â­
   - Code â†’ claude-haiku ($2.40/1M) â­
   - Research â†’ sonar ($0.20/1M) â­
   â†“
4. Antwort generieren
   â†“
5. Optional: Tests/Summaries mit claude-haiku
```

### **Kosten-Optimierung erreicht:**
- âœ… 94% gÃ¼nstiger bei Chats
- âœ… 73% gÃ¼nstiger bei Code-Tasks
- âœ… 98% gÃ¼nstiger bei Research
- âœ… Durchschnittlich 90-95% Ersparnis

### **QualitÃ¤t beibehalten:**
- âœ… Premium-Modelle verfÃ¼gbar
- âœ… Automatische Task-Optimierung
- âœ… Intelligente Fallbacks

---

## ğŸ“ CODE-REFERENZEN

```
Hauptlogik:           /app/backend/app/api/chat.py
Intelligente Agents:  /app/backend/app/core/intelligent_agents.py
Research-System:      /app/backend/app/core/coding_prompt.py
Multi-Agent:          /app/backend/app/core/multi_agent_orchestrator.py
Session Management:   /app/backend/app/api/session_management.py
AI Manager:           /app/backend/app/core/ai_manager.py
Model-Kosten:         /app/MODEL_COSTS.json
```

---

**Erstellt am:** 04.10.2025
**Version:** 2.0 (Nach Kostenoptimierung)
